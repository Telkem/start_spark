{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c678be-1d27-440c-ae2b-ab7d560c8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"miniproject1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2d52b2c-b02e-4b45-b16d-e6cca0ab59b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:////home/jovyan/work/start_spark/learning_spark_data/Crop_recommendation.csv'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "trip_data_path = os.path.join(cwd, 'learning_spark_data/Crop_recommendation.csv')\n",
    "trip_data_path\n",
    "file_path = f\"file:///{trip_data_path.replace(os.sep, '/')}\"\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "de125d52-b9c3-4817-9b75-584e3174dfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- N: integer (nullable = true)\n",
      " |-- P: integer (nullable = true)\n",
      " |-- K: integer (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- ph: double (nullable = true)\n",
      " |-- rainfall: double (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(file_path, inferSchema=True, header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4e25fc16-2fc7-45b5-93c4-59155aa80e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
      "|  N|  P|  K|temperature|   humidity|               ph|   rainfall|label|\n",
      "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
      "| 90| 42| 43|20.87974371|82.00274423|6.502985292000001|202.9355362| rice|\n",
      "| 85| 58| 41|21.77046169|80.31964408|      7.038096361|226.6555374| rice|\n",
      "| 60| 55| 44|23.00445915| 82.3207629|      7.840207144|263.9642476| rice|\n",
      "| 74| 35| 40|26.49109635|80.15836264|      6.980400905|242.8640342| rice|\n",
      "| 78| 42| 42|20.13017482|81.60487287|      7.628472891|262.7173405| rice|\n",
      "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a8adbf07-5dbd-4d95-aa7d-6c9602469886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------+---+--------+-----+\n",
      "|  N|  P|  K|temperature|humidity| ph|rainfall|label|\n",
      "+---+---+---+-----------+--------+---+--------+-----+\n",
      "|  0|  0|  0|          0|       0|  0|       0|    0|\n",
      "+---+---+---+-----------+--------+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, when, isnan\n",
    "null_counts = df.select(\n",
    "                    [\n",
    "                        sum(when(col(c).isNull() | isnan(c), 1).otherwise(0)).alias(c)\n",
    "                        for c in df.columns\n",
    "                    ]\n",
    "                )\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7d0a2b08-235b-49dd-9e53-895e9d5355fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "85537ccf-8f7b-49b6-bc05-c596e284a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5706c1bb-c75c-4653-a791-6ca8ccc7097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df.randomSplit([0.8,0.2], seed=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4afc6d0b-f6df-49ce-ac7a-e9e4a51bbe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol='label',outputCol = 'labelIndexer')\n",
    "stages += [labelIndexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "227abeea-2e2e-46ab-98b7-b43bb05f8552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_ec9f2ef1b4ad,\n",
       " VectorAssembler_4b4cb17fe71a,\n",
       " StandardScaler_56fb7376332d,\n",
       " VectorAssembler_d5ebcb6a1ffe,\n",
       " StandardScaler_737fd4a97883,\n",
       " VectorAssembler_d65173a6dbd6,\n",
       " StandardScaler_5c47917eff3a,\n",
       " VectorAssembler_83906c6a6bf2,\n",
       " StandardScaler_b811d6da7a52,\n",
       " VectorAssembler_3317a0069223,\n",
       " StandardScaler_3932966b61fd,\n",
       " VectorAssembler_78ac2345bca2,\n",
       " StandardScaler_07fcd7806962,\n",
       " VectorAssembler_c98b326aa56b,\n",
       " StandardScaler_3ee5cadf4cdb]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']\n",
    "for num in num_features:\n",
    "    num_assembler = VectorAssembler(inputCols=[num], outputCol=num+'_vector')\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol=num+'_scaled')\n",
    "    stages += [num_assembler, num_scaler]\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "887be9e5-2e7f-4051-a193-00b7cb4be25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N_scaled',\n",
       " 'P_scaled',\n",
       " 'K_scaled',\n",
       " 'temperature_scaled',\n",
       " 'humidity_scaled',\n",
       " 'ph_scaled',\n",
       " 'rainfall_scaled']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler_input = [num+'_scaled' for num in num_features]\n",
    "assembler_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6369d82-f301-4251-a842-3484be6e0818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_ec9f2ef1b4ad,\n",
       " VectorAssembler_4b4cb17fe71a,\n",
       " StandardScaler_56fb7376332d,\n",
       " VectorAssembler_d5ebcb6a1ffe,\n",
       " StandardScaler_737fd4a97883,\n",
       " VectorAssembler_d65173a6dbd6,\n",
       " StandardScaler_5c47917eff3a,\n",
       " VectorAssembler_83906c6a6bf2,\n",
       " StandardScaler_b811d6da7a52,\n",
       " VectorAssembler_3317a0069223,\n",
       " StandardScaler_3932966b61fd,\n",
       " VectorAssembler_78ac2345bca2,\n",
       " StandardScaler_07fcd7806962,\n",
       " VectorAssembler_c98b326aa56b,\n",
       " StandardScaler_3ee5cadf4cdb,\n",
       " VectorAssembler_ee2ec5dfc6c7]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols= assembler_input,\n",
    "    outputCol= 'feature_vector'\n",
    ")\n",
    "stages += [assembler]\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "bb750d55-2dd5-4cb3-ae5e-61e8bfe44a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- N: integer (nullable = true)\n",
      " |-- P: integer (nullable = true)\n",
      " |-- K: integer (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- ph: double (nullable = true)\n",
      " |-- rainfall: double (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- labelIndexer: double (nullable = false)\n",
      " |-- N_vector: vector (nullable = true)\n",
      " |-- N_scaled: vector (nullable = true)\n",
      " |-- P_vector: vector (nullable = true)\n",
      " |-- P_scaled: vector (nullable = true)\n",
      " |-- K_vector: vector (nullable = true)\n",
      " |-- K_scaled: vector (nullable = true)\n",
      " |-- temperature_vector: vector (nullable = true)\n",
      " |-- temperature_scaled: vector (nullable = true)\n",
      " |-- humidity_vector: vector (nullable = true)\n",
      " |-- humidity_scaled: vector (nullable = true)\n",
      " |-- ph_vector: vector (nullable = true)\n",
      " |-- ph_scaled: vector (nullable = true)\n",
      " |-- rainfall_vector: vector (nullable = true)\n",
      " |-- rainfall_scaled: vector (nullable = true)\n",
      " |-- feature_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "fitted_transform = pipeline.fit(train_data)\n",
    "vtrain_data = fitted_transform.transform(train_data)\n",
    "vtrain_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c867a9b7-cb60-4c6e-a281-69ae70176294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|      feature_vector|labelIndexer|\n",
      "+--------------------+------------+\n",
      "|[0.0,0.1494661232...|        17.0|\n",
      "|[0.0,0.3587186958...|         0.0|\n",
      "|[0.0,0.5081848190...|         5.0|\n",
      "+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vtrain_data.select('feature_vector', 'labelIndexer').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ed791e6f-119d-476f-b86f-c7b2a4ca0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8cf7bdd6-170c-4d62-a10e-b03176a3a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='feature_vector', labelCol='labelIndexer')\n",
    "lr_model = lr.fit(vtrain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "fbc969fe-2211-4102-a511-64480f4f06d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트데이터도 변환\n",
    "vtest_data = fitted_transform.transform(test_data)\n",
    "#테스트데이터로 예측\n",
    "pred = lr_model.transform(vtest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7bc1661e-3d0b-4cd9-9466-0ef49c84c08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|labelIndexer|prediction|\n",
      "+------------+----------+\n",
      "|         4.0|       4.0|\n",
      "|         7.0|       7.0|\n",
      "|         7.0|       7.0|\n",
      "|        15.0|      15.0|\n",
      "|        20.0|      20.0|\n",
      "+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.select('labelIndexer', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "562cb0f8-14be-4fcd-96c8-8bfdcb6bb8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "comp = pred.withColumn('correct', expr('case when labelIndexer = prediction then 1 else 0 end'))\n",
    "comp.where('correct=0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "94d6989d-c7ec-47f5-9062-a6b318f980a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+-----------+-----------------+------------------+---------+------------+----------+\n",
      "|  N|  P|  K|temperature|   humidity|               ph|          rainfall|    label|labelIndexer|prediction|\n",
      "+---+---+---+-----------+-----------+-----------------+------------------+---------+------------+----------+\n",
      "| 20| 68| 23|25.54960633|63.95425534|      7.707332484|        63.1830529|blackgram|        16.0|       0.0|\n",
      "| 31| 70| 77|20.88818675|14.32313811|      6.492546046|       90.46228334| chickpea|        13.0|      15.0|\n",
      "| 32| 73| 81|20.45078582|15.40312102|5.988992796000002|       92.68373702| chickpea|        13.0|      15.0|\n",
      "| 36| 58| 25|28.66024187| 59.3189118|8.399135957999999|       36.92629678|mothbeans|        21.0|       0.0|\n",
      "| 68| 52| 49|24.42561272|92.27749066|      6.577192175|       63.35298768|   papaya|        10.0|      19.0|\n",
      "| 78| 43| 42|21.32376327|83.00320459|7.283736617000001|       192.3197536|     rice|        11.0|       8.0|\n",
      "| 82| 48| 36|25.79351957|81.76904006|6.352076782999999|       193.2418382|     jute|         8.0|      11.0|\n",
      "| 89| 41| 38|23.12844351|74.68322732|6.344751947000001|199.83629130000003|     jute|         8.0|      11.0|\n",
      "| 93| 56| 42|23.85724032|82.22572988|      7.382762603|       195.0948311|     rice|        11.0|       8.0|\n",
      "+---+---+---+-----------+-----------+-----------------+------------------+---------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.select('N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall','label','labelIndexer','prediction').filter(col('labelIndexer') != col('prediction')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7cc84e3f-5c63-4324-8de1-af2cf4c727d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9803063457330415"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp.selectExpr('avg(correct) as accuracy').collect()[0]['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2d898e94-c291-4684-95b7-f08ebac8cd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lentil'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vtrain_data.select('label').filter(col('labelIndexer') == 0).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "577a19de-2d0b-4de4-8aa3-ace8e34bdf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lentil : 1.0\n",
      "maize : 1.0\n",
      "grapes : 1.0\n",
      "kidneybeans : 1.0\n",
      "rice : 0.9444444444444444\n",
      "coconut : 1.0\n",
      "mango : 1.0\n",
      "apple : 1.0\n",
      "banana : 1.0\n",
      "blackgram : 0.9523809523809523\n",
      "pomegranate : 1.0\n",
      "watermelon : 1.0\n",
      "cotton : 1.0\n",
      "papaya : 1.0\n",
      "coffee : 1.0\n",
      "jute : 0.875\n",
      "mungbean : 1.0\n",
      "muskmelon : 1.0\n",
      "orange : 1.0\n",
      "pigeonpeas : 1.0\n",
      "chickpea : 1.0\n",
      "mothbeans : 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "max_index = vtrain_data.select(max('labelIndexer')).collect()[0][0]\n",
    "for i in range(int(max_index) + 1):\n",
    "    acc = comp.filter(col('labelIndexer') == i).selectExpr('avg(correct) as accuracy').collect()[0]['accuracy']\n",
    "    crop_name = vtrain_data.select('label').filter(col('labelIndexer') == i).collect()[0][0]\n",
    "    print(f'{crop_name} : {acc}')\n",
    "# comp.filter(col('labelIndexer') == 0).selectExpr('avg(correct) as accuracy').collect()[0]['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "512b8c4e-f37a-4763-83da-4d2531062e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles = df.approxQuantile(\"rainfall\", [0.25, 0.75], 0.01)\n",
    "Q1 = quantiles[0]\n",
    "Q3 = quantiles[1]\n",
    "\n",
    "# 2. IQR 계산\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. 경계값 설정\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "drop_count = df.filter((col(\"rainfall\") < lower_bound) | (col(\"rainfall\") > upper_bound)).count()\n",
    "df_no_outliers = df.filter((col(\"rainfall\") >= lower_bound) & (col(\"rainfall\") <= upper_bound))\n",
    "drop_count\n",
    "# df_no_outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "04f17b5e-fabe-4d02-aced-6a9fc732dce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, stddev\n",
    "# 1. 평균 및 표준편차 계산\n",
    "mean_val = df.select(avg(\"rainfall\")).collect()[0][0]\n",
    "stddev_val = df.select(stddev(\"rainfall\")).collect()[0][0]\n",
    "\n",
    "# 2. Z-score 임계값 설정\n",
    "threshold = 3.0\n",
    "\n",
    "\n",
    "drop_count = df.filter(\n",
    "    (col(\"rainfall\") < (mean_val - threshold * stddev_val)) |\n",
    "    (col(\"rainfall\") > (mean_val + threshold * stddev_val))\n",
    ").count()\n",
    "# 3. 이상치 제거\n",
    "df_no_outliers_zscore = df.filter(\n",
    "    (col(\"rainfall\") >= (mean_val - threshold * stddev_val)) &\n",
    "    (col(\"rainfall\") <= (mean_val + threshold * stddev_val))\n",
    ")\n",
    "drop_count\n",
    "# df_no_outliers_zscore.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "29807ede-4639-4cc7-b361-4b2e9f467c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_val = df.select(avg(\"ph\")).collect()[0][0]\n",
    "stddev_val = df.select(stddev(\"ph\")).collect()[0][0]\n",
    "\n",
    "# 2. Z-score 임계값 설정\n",
    "threshold = 3.0\n",
    "\n",
    "\n",
    "drop_count = df.filter(\n",
    "    (col(\"ph\") < (mean_val - threshold * stddev_val)) |\n",
    "    (col(\"ph\") > (mean_val + threshold * stddev_val))\n",
    ").count()\n",
    "# 3. 이상치 제거\n",
    "df_no_outliers_zscore = df.filter(\n",
    "    (col(\"ph\") >= (mean_val - threshold * stddev_val)) &\n",
    "    (col(\"ph\") <= (mean_val + threshold * stddev_val))\n",
    ")\n",
    "drop_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "6a01765f-30d9-4619-ba2e-d39e2000198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_no_outliers_zscore.randomSplit([0.8,0.2], seed=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ab204e3c-df94-4289-8f0b-3ca9f783e39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']\n",
    "df_no_outliers_zscore = df\n",
    "for num in num_features:\n",
    "    mean_val = df_no_outliers_zscore.select(avg(num)).collect()[0][0]\n",
    "    stddev_val = df_no_outliers_zscore.select(stddev(num)).collect()[0][0]\n",
    "    \n",
    "    # 2. Z-score 임계값 설정\n",
    "    threshold = 3.0\n",
    "    \n",
    "    \n",
    "    # 3. 이상치 제거\n",
    "    df_no_outliers_zscore = df_no_outliers_zscore.filter(\n",
    "        (col(num) >= (mean_val - threshold * stddev_val)) &\n",
    "        (col(num) <= (mean_val + threshold * stddev_val))\n",
    "    )\n",
    "df.count() - df_no_outliers_zscore.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "14088900-4a1f-4156-bc4d-876285b4b865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N : 0.9803063457330415\n",
      "P : 0.9803063457330415\n",
      "K : 0.9839080459770115\n",
      "temperature : 0.9844789356984479\n",
      "humidity : 0.9803063457330415\n",
      "ph : 0.9889380530973452\n",
      "rainfall : 0.9845474613686535\n"
     ]
    }
   ],
   "source": [
    "num_features = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']\n",
    "df_no_outliers_zscore = df\n",
    "for num in num_features:\n",
    "    mean_val = df.select(avg(num)).collect()[0][0]\n",
    "    stddev_val = df.select(stddev(num)).collect()[0][0]\n",
    "    \n",
    "    # 2. Z-score 임계값 설정\n",
    "    threshold = 3.0\n",
    "    \n",
    "    \n",
    "    # 3. 이상치 제거\n",
    "    df_no_outliers_zscore = df.filter(\n",
    "        (col(num) >= (mean_val - threshold * stddev_val)) &\n",
    "        (col(num) <= (mean_val + threshold * stddev_val))\n",
    "    )\n",
    "    train_data, test_data = df_no_outliers_zscore.randomSplit([0.8,0.2], seed=64)\n",
    "    \n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    fitted_transform = pipeline.fit(train_data)\n",
    "    vtrain_data = fitted_transform.transform(train_data)\n",
    "    lr = LogisticRegression(featuresCol='feature_vector', labelCol='labelIndexer')\n",
    "    lr_model = lr.fit(vtrain_data)\n",
    "    #테스트데이터도 변환\n",
    "    vtest_data = fitted_transform.transform(test_data)\n",
    "    #테스트데이터로 예측\n",
    "    pred = lr_model.transform(vtest_data)\n",
    "    comp = pred.withColumn('correct', expr('case when labelIndexer = prediction then 1 else 0 end'))\n",
    "    comp.where('correct=0').count()\n",
    "    acc = comp.selectExpr('avg(correct) as accuracy').collect()[0]['accuracy']\n",
    "    print(f'{num} : {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "42552475-c105-4ba8-8481-3a0a5a035f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']\n",
    "df_no_outliers = df\n",
    "for num in num_features:\n",
    "    quantiles = df_no_outliers.approxQuantile(num, [0.25, 0.75], 0.01)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    \n",
    "    # 2. IQR 계산\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # 3. 경계값 설정\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df_no_outliers = df_no_outliers.filter((col(num) >= lower_bound) & (col(num) <= upper_bound))\n",
    "\n",
    "df.count() - df_no_outliers.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
