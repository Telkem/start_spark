{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01186a5-55ff-4282-8f3d-60d6523cf339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark 세션을 생성합니다.\n",
    "# \"miniproject1\"이라는 이름으로 Spark 애플리케이션을 시작하고, getOrCreate()를 사용하여 기존 세션이 있으면 재사용합니다.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"miniproject1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b1d632-fd00-4ae6-9487-4625056e5e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:////home/jovyan/work/start_spark/learning_spark_data/Crop_recommendation.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 파일의 경로를 설정합니다.\n",
    "# 현재 작업 디렉토리(cwd)를 기준으로 'learning_spark_data/Crop_recommendation.csv' 파일의 전체 경로를 만듭니다.\n",
    "# PySpark가 파일을 올바르게 찾을 수 있도록 'file:///' 접두사를 추가합니다.\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "trip_data_path = os.path.join(cwd, 'learning_spark_data/Crop_recommendation.csv')\n",
    "trip_data_path\n",
    "file_path = f\"file:///{trip_data_path.replace(os.sep, '/')}\"\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42f408c6-558a-4a5d-a393-3c79a80132c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- N: integer (nullable = true)\n",
      " |-- P: integer (nullable = true)\n",
      " |-- K: integer (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- ph: double (nullable = true)\n",
      " |-- rainfall: double (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일을 읽어 Spark DataFrame을 생성합니다.\n",
    "# inferSchema=True: 데이터의 스키마(데이터 타입)를 자동으로 추론합니다.\n",
    "# header=True: 첫 번째 행을 헤더로 인식합니다.\n",
    "# printSchema()를 사용하여 DataFrame의 스키마를 출력합니다.\n",
    "df = spark.read.csv(file_path, inferSchema=True, header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c81662-cc89-4f16-b48a-d2bb5500e9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
      "|  N|  P|  K|temperature|   humidity|               ph|   rainfall|label|\n",
      "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
      "| 90| 42| 43|20.87974371|82.00274423|6.502985292000001|202.9355362| rice|\n",
      "| 85| 58| 41|21.77046169|80.31964408|      7.038096361|226.6555374| rice|\n",
      "| 60| 55| 44|23.00445915| 82.3207629|      7.840207144|263.9642476| rice|\n",
      "| 74| 35| 40|26.49109635|80.15836264|      6.980400905|242.8640342| rice|\n",
      "| 78| 42| 42|20.13017482|81.60487287|      7.628472891|262.7173405| rice|\n",
      "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame의 상위 5개 행을 출력하여 데이터 구조를 확인합니다.\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94543c56-8e5c-4a4b-a042-fa6569be5cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------+---+--------+-----+\n",
      "|  N|  P|  K|temperature|humidity| ph|rainfall|label|\n",
      "+---+---+---+-----------+--------+---+--------+-----+\n",
      "|  0|  0|  0|          0|       0|  0|       0|    0|\n",
      "+---+---+---+-----------+--------+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 각 컬럼에 Null 또는 NaN(숫자 형태의 결측값)이 있는지 확인하고 개수를 계산합니다.\n",
    "# isnan() 함수는 숫자형 컬럼에 대해서만 작동하므로, isNull()과 함께 사용합니다.\n",
    "# 결과 DataFrame에는 각 컬럼의 결측값 개수가 포함됩니다.\n",
    "from pyspark.sql.functions import col, sum, when, isnan\n",
    "null_counts = df.select(\n",
    "                    [\n",
    "                        sum(when(col(c).isNull() | isnan(c), 1).otherwise(0)).alias(c)\n",
    "                        for c in df.columns\n",
    "                    ]\n",
    "                )\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1f63189-adbc-46e6-9b72-05b6849bd531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML 모델 구축에 필요한 PySpark ML 라이브러리를 임포트합니다.\n",
    "# LogisticRegression: 로지스틱 회귀 모델\n",
    "# BinaryClassificationEvaluator: 이진 분류 모델 평가 도구 (이 노트북에서는 사용되지 않았지만 임포트되어 있음)\n",
    "# VectorAssembler: 여러 컬럼을 단일 벡터 컬럼으로 변환\n",
    "# StringIndexer: 문자열 라벨을 숫자 인덱스로 변환\n",
    "# OneHotEncoder: 원-핫 인코딩 (이 노트북에서는 사용되지 않았음)\n",
    "# StandardScaler: 데이터 표준화 (평균 0, 분산 1)\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60718792-8307-4b5a-b929-dc6ab619dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 훈련 세트(80%)와 테스트 세트(20%)로 무작위 분할합니다.\n",
    "# seed=64: 재현성을 위해 시드를 고정합니다.\n",
    "train_data, test_data = df.randomSplit([0.8,0.2], seed=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06cb8ca4-b46b-48ca-8e82-559d0cf47ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인(Pipeline)에 사용할 스테이지(단계)를 저장할 빈 리스트를 생성합니다.\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ca72ed2-61fa-4fcb-8192-5a2d48661c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'label' 컬럼(문자열)을 'labelIndexer'라는 숫자형 인덱스로 변환하는 StringIndexer 스테이지를 정의하고 파이프라인에 추가합니다.\n",
    "labelIndexer = StringIndexer(inputCol='label',outputCol = 'labelIndexer')\n",
    "stages += [labelIndexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5802c6da-13c2-4f95-b1fe-1b3689b7106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자형 특성들에 대해 표준화(Standardization)를 수행하는 파이프라인 스테이지를 생성합니다.\n",
    "# num_features 리스트에 있는 각 컬럼에 대해 VectorAssembler와 StandardScaler를 적용합니다.\n",
    "# VectorAssembler: 각 숫자 컬럼을 단일 벡터로 만듭니다.\n",
    "# StandardScaler: 해당 벡터를 표준화(Z-score normalization)합니다.\n",
    "num_features = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']\n",
    "for num in num_features:\n",
    "    num_assembler = VectorAssembler(inputCols=[num], outputCol=num+'_vector')\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol=num+'_scaled')\n",
    "    stages += [num_assembler, num_scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92d8c2da-84be-4fd1-a075-82dce5358046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화된 모든 숫자 특성 컬럼의 이름을 assembler_input 리스트에 저장합니다.\n",
    "assembler_input = [num+'_scaled' for num in num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff20a499-d32e-4cc0-91bc-6a5174e66b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준화된 모든 특성 컬럼들을 'feature_vector'라는 하나의 벡터 컬럼으로 합치는 VectorAssembler 스테이지를 정의하고 파이프라인에 추가합니다.\n",
    "assembler = VectorAssembler(\n",
    "    inputCols= assembler_input,\n",
    "    outputCol= 'feature_vector'\n",
    ")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d581ab9-1c8e-4979-9671-6c7ad41536f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서 정의한 모든 스테이지(StringIndexer, VectorAssembler, StandardScaler)를 포함하는 PySpark Pipeline을 생성합니다.\n",
    "# fit()을 사용하여 훈련 데이터에 파이프라인을 학습시키고, transform()으로 훈련 데이터에 변환을 적용합니다.\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "fitted_transform = pipeline.fit(train_data)\n",
    "vtrain_data = fitted_transform.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceecdb8b-f8e0-46c7-9edf-2a2645404b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로지스틱 회귀 모델 객체를 생성하고, 훈련 데이터에 맞춰 모델을 학습시킵니다.\n",
    "# featuresCol='feature_vector': 특성 컬럼으로 사용할 컬럼을 지정합니다.\n",
    "# labelCol='labelIndexer': 라벨 컬럼으로 사용할 컬럼을 지정합니다.\n",
    "lr = LogisticRegression(featuresCol='feature_vector', labelCol='labelIndexer')\n",
    "lr_model = lr.fit(vtrain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2110f104-84b8-440d-8167-5e617242cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터에도 훈련 데이터에 학습된 파이프라인 변환을 적용합니다.\n",
    "vtest_data = fitted_transform.transform(test_data)\n",
    "# 학습된 로지스틱 회귀 모델을 사용하여 테스트 데이터에 대한 예측을 수행합니다.\n",
    "pred = lr_model.transform(vtest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37702227-d1f4-4545-9290-a046b711b26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|labelIndexer|prediction|\n",
      "+------------+----------+\n",
      "|         4.0|       4.0|\n",
      "|         7.0|       7.0|\n",
      "|         7.0|       7.0|\n",
      "|        15.0|      15.0|\n",
      "|        20.0|      20.0|\n",
      "+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예측 결과 DataFrame에서 라벨 인덱스와 예측값을 상위 5개 출력하여 확인합니다.\n",
    "pred.select('labelIndexer', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f505cc70-074b-46fe-bafd-8810a1b0e5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측값과 실제 라벨이 일치하는지 비교하는 'correct' 컬럼을 생성합니다.\n",
    "# 일치하면 1, 불일치하면 0으로 설정합니다.\n",
    "# 예측이 틀린 경우(correct=0)의 개수를 계산하여 출력합니다.\n",
    "from pyspark.sql.functions import expr\n",
    "comp = pred.withColumn('correct', expr('case when labelIndexer = prediction then 1 else 0 end'))\n",
    "comp.where('correct=0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ee0e7b3-ae62-4c19-a1e2-df29bd6a5a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9803063457330415"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 정확도(accuracy)를 계산합니다.\n",
    "# 'correct' 컬럼의 평균을 계산하여 정확도를 나타냅니다.\n",
    "comp.selectExpr('avg(correct) as accuracy').collect()[0]['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07ff165b-3934-40a2-9e42-2ab785a34f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lentil : 1.0\n",
      "maize : 1.0\n",
      "orange : 1.0\n",
      "grapes : 1.0\n",
      "mango : 1.0\n",
      "muskmelon : 1.0\n",
      "pomegranate : 1.0\n",
      "coconut : 1.0\n",
      "jute : 0.9\n",
      "cotton : 1.0\n",
      "papaya : 0.9523809523809523\n",
      "rice : 0.9047619047619048\n",
      "banana : 1.0\n",
      "chickpea : 0.9090909090909091\n",
      "pigeonpeas : 1.0\n",
      "kidneybeans : 1.0\n",
      "blackgram : 0.9583333333333334\n",
      "coffee : 1.0\n",
      "mungbean : 1.0\n",
      "watermelon : 1.0\n",
      "apple : 1.0\n",
      "mothbeans : 0.9642857142857143\n"
     ]
    }
   ],
   "source": [
    "# 각 작물(label)별 정확도를 계산하여 출력합니다.\n",
    "# 먼저 최대 'labelIndexer' 값을 찾습니다.\n",
    "# 각 라벨 인덱스(i)에 대해 필터링하고 'correct' 컬럼의 평균을 계산합니다.\n",
    "# 해당 인덱스에 해당하는 작물 이름을 찾아 정확도와 함께 출력합니다.\n",
    "from pyspark.sql.functions import max\n",
    "max_index = vtrain_data.select(max('labelIndexer')).collect()[0][0]\n",
    "for i in range(int(max_index) + 1):\n",
    "    acc = comp.filter(col('labelIndexer') == i).selectExpr('avg(correct) as accuracy').collect()[0]['accuracy']\n",
    "    crop_name = vtrain_data.select('label').filter(col('labelIndexer') == i).collect()[0][0]\n",
    "    print(f'{crop_name} : {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da5d964f-425d-451c-bcab-a54c008a2a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pH 컬럼의 이상치를 Z-score 방법을 사용하여 제거하는 과정입니다.\n",
    "# 1. pH 컬럼의 평균(mean_val)과 표준편차(stddev_val)를 계산합니다.\n",
    "# 2. Z-score 임계값(threshold)을 3.0으로 설정합니다 (평균에서 3 표준편차 이상 떨어진 값).\n",
    "# 3. Z-score 임계값을 벗어나는 이상치의 개수를 계산합니다.\n",
    "# 4. 이상치를 제거한 새로운 DataFrame(df_no_outliers_zscore)을 생성합니다.\n",
    "from pyspark.sql.functions import col, avg, stddev\n",
    "mean_val = df.select(avg(\"ph\")).collect()[0][0]\n",
    "stddev_val = df.select(stddev(\"ph\")).collect()[0][0]\n",
    "\n",
    "# 2. Z-score 임계값 설정\n",
    "threshold = 3.0\n",
    "\n",
    "\n",
    "drop_count = df.filter(\n",
    "    (col(\"ph\") < (mean_val - threshold * stddev_val)) |\n",
    "    (col(\"ph\") > (mean_val + threshold * stddev_val))\n",
    ").count()\n",
    "# 3. 이상치 제거\n",
    "df_no_outliers_zscore = df.filter(\n",
    "    (col(\"ph\") >= (mean_val - threshold * stddev_val)) &\n",
    "    (col(\"ph\") <= (mean_val + threshold * stddev_val))\n",
    ")\n",
    "drop_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "691283a5-18df-472f-b67a-f28d6db6b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치가 제거된 데이터(df_no_outliers_zscore)를 훈련 세트(80%)와 테스트 세트(20%)로 무작위 분할합니다.\n",
    "# seed=64: 재현성을 위해 시드를 고정합니다.\n",
    "ztrain_data, ztest_data = df_no_outliers_zscore.randomSplit([0.8,0.2], seed=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54d8ff5e-05cd-4798-b9f4-ae3f84156fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치가 제거된 훈련 데이터(ztrain_data)를 사용하여 파이프라인을 다시 학습시키고 변환을 적용합니다.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "fitted_transform = pipeline.fit(ztrain_data)\n",
    "vztrain_data = fitted_transform.transform(ztrain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad26b68e-7cc1-4ef1-a104-85eab2b34648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로지스틱 회귀 모델을 생성하고, 이상치가 제거된 훈련 데이터(vztrain_data)로 모델을 다시 학습시킵니다.\n",
    "lr = LogisticRegression(featuresCol='feature_vector', labelCol='labelIndexer')\n",
    "lr_model = lr.fit(vztrain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37bc4b3c-748f-45fc-b914-42c4573a67d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치가 제거된 테스트 데이터(ztest_data)에 파이프라인 변환을 적용하고, 모델을 사용하여 예측을 수행합니다.\n",
    "vztest_data = fitted_transform.transform(ztest_data)\n",
    "zpred = lr_model.transform(vztest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34300886-4b75-4b96-a0d3-8734eb20d35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|labelIndexer|prediction|\n",
      "+------------+----------+\n",
      "|         6.0|       6.0|\n",
      "|         5.0|       5.0|\n",
      "|         5.0|       5.0|\n",
      "|         3.0|       3.0|\n",
      "|         7.0|       7.0|\n",
      "+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 이상치가 제거된 데이터로 예측한 결과의 상위 5개 라벨 인덱스와 예측값을 출력합니다.\n",
    "zpred.select('labelIndexer', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a7ba5cf-2794-4edb-add3-eb6a9b979457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이상치가 제거된 데이터로 예측한 결과에서, 예측이 틀린 경우(correct=0)의 개수를 계산합니다.\n",
    "from pyspark.sql.functions import expr\n",
    "zcomp = zpred.withColumn('correct', expr('case when labelIndexer = prediction then 1 else 0 end'))\n",
    "zcomp.where('correct=0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11a4e4e1-4d2d-4c67-8343-425758a93ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9889380530973452"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이상치 제거 후의 전체 정확도를 계산하여 출력합니다.\n",
    "zcomp.selectExpr('avg(correct) as accuracy').collect()[0]['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19a73f31-6b5a-4a65-87ab-a2b7d1470f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lentil : 1.0\n",
      "maize : 1.0\n",
      "grapes : 1.0\n",
      "kidneybeans : 1.0\n",
      "rice : 0.9444444444444444\n",
      "coconut : 1.0\n",
      "mango : 1.0\n",
      "apple : 1.0\n",
      "banana : 1.0\n",
      "blackgram : 0.9523809523809523\n",
      "pomegranate : 1.0\n",
      "watermelon : 1.0\n",
      "cotton : 1.0\n",
      "papaya : 1.0\n",
      "coffee : 1.0\n",
      "jute : 0.875\n",
      "mungbean : 1.0\n",
      "muskmelon : 1.0\n",
      "orange : 1.0\n",
      "pigeonpeas : 1.0\n",
      "chickpea : 1.0\n",
      "mothbeans : 1.0\n"
     ]
    }
   ],
   "source": [
    "# 이상치 제거 후의 각 작물별 정확도를 계산하여 출력합니다.\n",
    "for i in range(int(max_index) + 1):\n",
    "    acc = zcomp.filter(col('labelIndexer') == i).selectExpr('avg(correct) as accuracy').collect()[0]['accuracy']\n",
    "    crop_name = vztrain_data.select('label').filter(col('labelIndexer') == i).collect()[0][0]\n",
    "    print(f'{crop_name} : {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93cf1ba2-4f1e-4b9b-85bb-c9ecf931ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용된 spark 세션을 종료합니다.\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
