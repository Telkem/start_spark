{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6a3428-e29c-4ccc-8c5e-ce6b4921c170",
   "metadata": {},
   "source": [
    "# 조건별 작물 추천 모델링 만들기\n",
    "데이터 출처 : https://www.kaggle.com/datasets/madhuraatmarambhagat/crop-recommendation-dataset?select=Crop_recommendation.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c678be-1d27-440c-ae2b-ab7d560c8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"miniproject1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2d52b2c-b02e-4b45-b16d-e6cca0ab59b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:////home/jovyan/work/start_spark/learning_spark_data/Crop_recommendation.csv'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'file:////home/jovyan/work/start_spark/learning_spark_data/Crop_recommendation.csv'\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "de125d52-b9c3-4817-9b75-584e3174dfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- N: integer (nullable = true)\n",
      " |-- P: integer (nullable = true)\n",
      " |-- K: integer (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- ph: double (nullable = true)\n",
      " |-- rainfall: double (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(file_path, inferSchema=True, header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4e25fc16-2fc7-45b5-93c4-59155aa80e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
      "|  N|  P|  K|temperature|   humidity|               ph|   rainfall|label|\n",
      "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
      "| 90| 42| 43|20.87974371|82.00274423|6.502985292000001|202.9355362| rice|\n",
      "| 85| 58| 41|21.77046169|80.31964408|      7.038096361|226.6555374| rice|\n",
      "| 60| 55| 44|23.00445915| 82.3207629|      7.840207144|263.9642476| rice|\n",
      "| 74| 35| 40|26.49109635|80.15836264|      6.980400905|242.8640342| rice|\n",
      "| 78| 42| 42|20.13017482|81.60487287|      7.628472891|262.7173405| rice|\n",
      "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a8adbf07-5dbd-4d95-aa7d-6c9602469886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+--------+---+--------+-----+\n",
      "|  N|  P|  K|temperature|humidity| ph|rainfall|label|\n",
      "+---+---+---+-----------+--------+---+--------+-----+\n",
      "|  0|  0|  0|          0|       0|  0|       0|    0|\n",
      "+---+---+---+-----------+--------+---+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, when, isnan\n",
    "null_counts = df.select(\n",
    "                    [\n",
    "                        sum(when(col(c).isNull() | isnan(c), 1).otherwise(0)).alias(c)\n",
    "                        for c in df.columns\n",
    "                    ]\n",
    "                )\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7d0a2b08-235b-49dd-9e53-895e9d5355fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "85537ccf-8f7b-49b6-bc05-c596e284a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5706c1bb-c75c-4653-a791-6ca8ccc7097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df.randomSplit([0.8,0.2], seed=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4afc6d0b-f6df-49ce-ac7a-e9e4a51bbe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol='label',outputCol = 'labelIndexer')\n",
    "stages += [labelIndexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "227abeea-2e2e-46ab-98b7-b43bb05f8552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_ec9f2ef1b4ad,\n",
       " VectorAssembler_4b4cb17fe71a,\n",
       " StandardScaler_56fb7376332d,\n",
       " VectorAssembler_d5ebcb6a1ffe,\n",
       " StandardScaler_737fd4a97883,\n",
       " VectorAssembler_d65173a6dbd6,\n",
       " StandardScaler_5c47917eff3a,\n",
       " VectorAssembler_83906c6a6bf2,\n",
       " StandardScaler_b811d6da7a52,\n",
       " VectorAssembler_3317a0069223,\n",
       " StandardScaler_3932966b61fd,\n",
       " VectorAssembler_78ac2345bca2,\n",
       " StandardScaler_07fcd7806962,\n",
       " VectorAssembler_c98b326aa56b,\n",
       " StandardScaler_3ee5cadf4cdb]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']\n",
    "for num in num_features:\n",
    "    num_assembler = VectorAssembler(inputCols=[num], outputCol=num+'_vector')\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol=num+'_scaled')\n",
    "    stages += [num_assembler, num_scaler]\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "887be9e5-2e7f-4051-a193-00b7cb4be25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N_scaled',\n",
       " 'P_scaled',\n",
       " 'K_scaled',\n",
       " 'temperature_scaled',\n",
       " 'humidity_scaled',\n",
       " 'ph_scaled',\n",
       " 'rainfall_scaled']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler_input = [num+'_scaled' for num in num_features]\n",
    "assembler_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6369d82-f301-4251-a842-3484be6e0818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_ec9f2ef1b4ad,\n",
       " VectorAssembler_4b4cb17fe71a,\n",
       " StandardScaler_56fb7376332d,\n",
       " VectorAssembler_d5ebcb6a1ffe,\n",
       " StandardScaler_737fd4a97883,\n",
       " VectorAssembler_d65173a6dbd6,\n",
       " StandardScaler_5c47917eff3a,\n",
       " VectorAssembler_83906c6a6bf2,\n",
       " StandardScaler_b811d6da7a52,\n",
       " VectorAssembler_3317a0069223,\n",
       " StandardScaler_3932966b61fd,\n",
       " VectorAssembler_78ac2345bca2,\n",
       " StandardScaler_07fcd7806962,\n",
       " VectorAssembler_c98b326aa56b,\n",
       " StandardScaler_3ee5cadf4cdb,\n",
       " VectorAssembler_ee2ec5dfc6c7]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols= assembler_input,\n",
    "    outputCol= 'feature_vector'\n",
    ")\n",
    "stages += [assembler]\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "bb750d55-2dd5-4cb3-ae5e-61e8bfe44a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- N: integer (nullable = true)\n",
      " |-- P: integer (nullable = true)\n",
      " |-- K: integer (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- ph: double (nullable = true)\n",
      " |-- rainfall: double (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- labelIndexer: double (nullable = false)\n",
      " |-- N_vector: vector (nullable = true)\n",
      " |-- N_scaled: vector (nullable = true)\n",
      " |-- P_vector: vector (nullable = true)\n",
      " |-- P_scaled: vector (nullable = true)\n",
      " |-- K_vector: vector (nullable = true)\n",
      " |-- K_scaled: vector (nullable = true)\n",
      " |-- temperature_vector: vector (nullable = true)\n",
      " |-- temperature_scaled: vector (nullable = true)\n",
      " |-- humidity_vector: vector (nullable = true)\n",
      " |-- humidity_scaled: vector (nullable = true)\n",
      " |-- ph_vector: vector (nullable = true)\n",
      " |-- ph_scaled: vector (nullable = true)\n",
      " |-- rainfall_vector: vector (nullable = true)\n",
      " |-- rainfall_scaled: vector (nullable = true)\n",
      " |-- feature_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "fitted_transform = pipeline.fit(train_data)\n",
    "vtrain_data = fitted_transform.transform(train_data)\n",
    "vtrain_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c867a9b7-cb60-4c6e-a281-69ae70176294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|      feature_vector|labelIndexer|\n",
      "+--------------------+------------+\n",
      "|[0.0,0.1494661232...|        17.0|\n",
      "|[0.0,0.3587186958...|         0.0|\n",
      "|[0.0,0.5081848190...|         5.0|\n",
      "+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vtrain_data.select('feature_vector', 'labelIndexer').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ed791e6f-119d-476f-b86f-c7b2a4ca0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "8cf7bdd6-170c-4d62-a10e-b03176a3a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='feature_vector', labelCol='labelIndexer')\n",
    "lr_model = lr.fit(vtrain_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "fbc969fe-2211-4102-a511-64480f4f06d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트데이터도 변환\n",
    "vtest_data = fitted_transform.transform(test_data)\n",
    "#테스트데이터로 예측\n",
    "pred = lr_model.transform(vtest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "7bc1661e-3d0b-4cd9-9466-0ef49c84c08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|labelIndexer|prediction|\n",
      "+------------+----------+\n",
      "|         7.0|       7.0|\n",
      "|         5.0|       5.0|\n",
      "|         5.0|       5.0|\n",
      "|         3.0|       3.0|\n",
      "|         8.0|       8.0|\n",
      "+------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.select('labelIndexer', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "562cb0f8-14be-4fcd-96c8-8bfdcb6bb8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "comp = pred.withColumn('correct', expr('case when labelIndexer = prediction then 1 else 0 end'))\n",
    "comp.where('correct=0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "94d6989d-c7ec-47f5-9062-a6b318f980a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+-----------------+------------------+-----------+-----------+------------+----------+\n",
      "|  N|  P|  K|temperature|         humidity|                ph|   rainfall|      label|labelIndexer|prediction|\n",
      "+---+---+---+-----------+-----------------+------------------+-----------+-----------+------------+----------+\n",
      "| 11| 78| 22|23.89756791|      22.74378977|       5.940546818|112.6616435|kidneybeans|         3.0|      17.0|\n",
      "| 18| 74| 15| 24.9035819|      22.27512704|        5.70836603|146.4727237|kidneybeans|         3.0|      17.0|\n",
      "| 18| 79| 20|20.27514686|       23.2353604|       5.877347515|139.7521543|kidneybeans|         3.0|      17.0|\n",
      "| 22| 55| 20|33.95309131|      69.96100028|       7.423530351|61.16350463|  blackgram|         9.0|      15.0|\n",
      "| 63| 37| 43|23.41798979|      85.08640476| 6.661957897000001|185.7446728|       jute|         2.0|      20.0|\n",
      "| 63| 41| 45|25.29781791|86.88705350000002|7.1219335789999985|196.6249511|       jute|         2.0|      20.0|\n",
      "| 83| 60| 36|25.59704938|      80.14509262|       6.903985986| 200.834898|       rice|        20.0|       2.0|\n",
      "| 89| 52| 42|23.09433785|      81.45139295|        6.14132902|196.6587013|       jute|         2.0|      20.0|\n",
      "| 91| 35| 38| 24.8972823|      80.52586088|        6.13428721|183.6793207|       rice|        20.0|       2.0|\n",
      "| 91| 41| 37|24.48556447|      83.20630007| 6.132570522999999|192.2316221|       jute|         2.0|      20.0|\n",
      "| 99| 41| 36|24.45802087|      82.74835604|       6.738652179|182.5616319|       rice|        20.0|       2.0|\n",
      "+---+---+---+-----------+-----------------+------------------+-----------+-----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.select('N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall','label','labelIndexer','prediction').filter(col('labelIndexer') != col('prediction')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "7cc84e3f-5c63-4324-8de1-af2cf4c727d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9753363228699552"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp.selectExpr('avg(correct) as accuracy').collect()[0]['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2d898e94-c291-4684-95b7-f08ebac8cd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lentil'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vtrain_data.select('label').filter(col('labelIndexer') == 0).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "577a19de-2d0b-4de4-8aa3-ace8e34bdf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lentil : 1.0\n",
      "grapes : 1.0\n",
      "jute : 0.75\n",
      "kidneybeans : 0.8235294117647058\n",
      "maize : 1.0\n",
      "coconut : 1.0\n",
      "cotton : 1.0\n",
      "mango : 1.0\n",
      "apple : 1.0\n",
      "blackgram : 0.9523809523809523\n",
      "pomegranate : 1.0\n",
      "muskmelon : 1.0\n",
      "watermelon : 1.0\n",
      "banana : 1.0\n",
      "papaya : 1.0\n",
      "mungbean : 1.0\n",
      "orange : 1.0\n",
      "pigeonpeas : 1.0\n",
      "chickpea : 1.0\n",
      "coffee : 1.0\n",
      "rice : 0.8\n",
      "mothbeans : 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "max_index = vtrain_data.select(max('labelIndexer')).collect()[0][0]\n",
    "for i in range(int(max_index) + 1):\n",
    "    acc = comp.filter(col('labelIndexer') == i).selectExpr('avg(correct) as accuracy').collect()[0]['accuracy']\n",
    "    crop_name = vtrain_data.select('label').filter(col('labelIndexer') == i).collect()[0][0]\n",
    "    print(f'{crop_name} : {acc}')\n",
    "# comp.filter(col('labelIndexer') == 0).selectExpr('avg(correct) as accuracy').collect()[0]['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "512b8c4e-f37a-4763-83da-4d2531062e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles = df.approxQuantile(\"rainfall\", [0.25, 0.75], 0.01)\n",
    "Q1 = quantiles[0]\n",
    "Q3 = quantiles[1]\n",
    "\n",
    "# 2. IQR 계산\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 3. 경계값 설정\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "drop_count = df.filter((col(\"rainfall\") < lower_bound) | (col(\"rainfall\") > upper_bound)).count()\n",
    "df_no_outliers = df.filter((col(\"rainfall\") >= lower_bound) & (col(\"rainfall\") <= upper_bound))\n",
    "drop_count\n",
    "# df_no_outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "04f17b5e-fabe-4d02-aced-6a9fc732dce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg, stddev\n",
    "# 1. 평균 및 표준편차 계산\n",
    "mean_val = df.select(avg(\"rainfall\")).collect()[0][0]\n",
    "stddev_val = df.select(stddev(\"rainfall\")).collect()[0][0]\n",
    "\n",
    "# 2. Z-score 임계값 설정\n",
    "threshold = 3.0\n",
    "\n",
    "\n",
    "drop_count = df.filter(\n",
    "    (col(\"rainfall\") < (mean_val - threshold * stddev_val)) |\n",
    "    (col(\"rainfall\") > (mean_val + threshold * stddev_val))\n",
    ").count()\n",
    "# 3. 이상치 제거\n",
    "df_no_outliers_zscore = df.filter(\n",
    "    (col(\"rainfall\") >= (mean_val - threshold * stddev_val)) &\n",
    "    (col(\"rainfall\") <= (mean_val + threshold * stddev_val))\n",
    ")\n",
    "drop_count\n",
    "# df_no_outliers_zscore.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "29807ede-4639-4cc7-b361-4b2e9f467c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_val = df.select(avg(\"ph\")).collect()[0][0]\n",
    "stddev_val = df.select(stddev(\"ph\")).collect()[0][0]\n",
    "\n",
    "# 2. Z-score 임계값 설정\n",
    "threshold = 3.0\n",
    "\n",
    "\n",
    "drop_count = df.filter(\n",
    "    (col(\"ph\") < (mean_val - threshold * stddev_val)) |\n",
    "    (col(\"ph\") > (mean_val + threshold * stddev_val))\n",
    ").count()\n",
    "# 3. 이상치 제거\n",
    "df_no_outliers_zscore = df.filter(\n",
    "    (col(\"ph\") >= (mean_val - threshold * stddev_val)) &\n",
    "    (col(\"ph\") <= (mean_val + threshold * stddev_val))\n",
    ")\n",
    "drop_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6a01765f-30d9-4619-ba2e-d39e2000198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_no_outliers_zscore.randomSplit([0.8,0.2], seed=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ab204e3c-df94-4289-8f0b-3ca9f783e39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = ['ph', 'rainfall']\n",
    "df_no_outliers_zscore = df\n",
    "for num in num_features:\n",
    "    mean_val = df_no_outliers_zscore.select(avg(num)).collect()[0][0]\n",
    "    stddev_val = df_no_outliers_zscore.select(stddev(num)).collect()[0][0]\n",
    "    \n",
    "    # 2. Z-score 임계값 설정\n",
    "    threshold = 3.0\n",
    "    \n",
    "    \n",
    "    # 3. 이상치 제거\n",
    "    df_no_outliers_zscore = df_no_outliers_zscore.filter(\n",
    "        (col(num) >= (mean_val - threshold * stddev_val)) &\n",
    "        (col(num) <= (mean_val + threshold * stddev_val))\n",
    "    )\n",
    "df.count() - df_no_outliers_zscore.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "14088900-4a1f-4156-bc4d-876285b4b865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N : 0.9803063457330415\n",
      "P : 0.9803063457330415\n",
      "K : 0.9839080459770115\n",
      "temperature : 0.9844789356984479\n",
      "humidity : 0.9803063457330415\n",
      "ph : 0.9889380530973452\n",
      "rainfall : 0.9845474613686535\n"
     ]
    }
   ],
   "source": [
    "num_features = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']\n",
    "df_no_outliers_zscore = df\n",
    "for num in num_features:\n",
    "    mean_val = df.select(avg(num)).collect()[0][0]\n",
    "    stddev_val = df.select(stddev(num)).collect()[0][0]\n",
    "    \n",
    "    # 2. Z-score 임계값 설정\n",
    "    threshold = 3.0\n",
    "    \n",
    "    \n",
    "    # 3. 이상치 제거\n",
    "    df_no_outliers_zscore = df.filter(\n",
    "        (col(num) >= (mean_val - threshold * stddev_val)) &\n",
    "        (col(num) <= (mean_val + threshold * stddev_val))\n",
    "    )\n",
    "    train_data, test_data = df_no_outliers_zscore.randomSplit([0.8,0.2], seed=64)\n",
    "    \n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    fitted_transform = pipeline.fit(train_data)\n",
    "    vtrain_data = fitted_transform.transform(train_data)\n",
    "    lr = LogisticRegression(featuresCol='feature_vector', labelCol='labelIndexer')\n",
    "    lr_model = lr.fit(vtrain_data)\n",
    "    #테스트데이터도 변환\n",
    "    vtest_data = fitted_transform.transform(test_data)\n",
    "    #테스트데이터로 예측\n",
    "    pred = lr_model.transform(vtest_data)\n",
    "    comp = pred.withColumn('correct', expr('case when labelIndexer = prediction then 1 else 0 end'))\n",
    "    comp.where('correct=0').count()\n",
    "    acc = comp.selectExpr('avg(correct) as accuracy').collect()[0]['accuracy']\n",
    "    print(f'{num} : {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "42552475-c105-4ba8-8481-3a0a5a035f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']\n",
    "df_no_outliers = df\n",
    "for num in num_features:\n",
    "    quantiles = df_no_outliers.approxQuantile(num, [0.25, 0.75], 0.01)\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    \n",
    "    # 2. IQR 계산\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # 3. 경계값 설정\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df_no_outliers = df_no_outliers.filter((col(num) >= lower_bound) & (col(num) <= upper_bound))\n",
    "\n",
    "df.count() - df_no_outliers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "dd028d70-ada4-4eb8-92b6-031204b6a16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ph 평균: 6.469480065256369, ph 표준편차: 0.7739376880298732\n",
      "ph 상한선: 8.791293129345988, ph 하한선: 4.14766700116675\n",
      "원래 데이터 수: 2200, 'ph' 이상치 제거 후 데이터 수: 2170\n",
      "모델 정확도 (Random Forest, ph Z-score 이상치 제거): 0.9712793733681462\n",
      "\n",
      "=== 작물별 정확도 ===\n",
      "작물 'apple': 1.0\n",
      "작물 'banana': 1.0\n",
      "작물 'blackgram': 1.0\n",
      "작물 'chickpea': 1.0\n",
      "작물 'coconut': 1.0\n",
      "작물 'coffee': 1.0\n",
      "작물 'cotton': 1.0\n",
      "작물 'grapes': 1.0\n",
      "작물 'jute': 1.0\n",
      "작물 'kidneybeans': 1.0\n",
      "작물 'lentil': 1.0\n",
      "작물 'maize': 1.0\n",
      "작물 'mango': 1.0\n",
      "작물 'mothbeans': 0.6363636363636364\n",
      "작물 'mungbean': 1.0\n",
      "작물 'muskmelon': 1.0\n",
      "작물 'orange': 1.0\n",
      "작물 'papaya': 1.0\n",
      "작물 'pigeonpeas': 0.75\n",
      "작물 'pomegranate': 1.0\n",
      "작물 'rice': 0.8571428571428571\n",
      "작물 'watermelon': 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# 1. 'ph' 컬럼 이상치 제거 (Z-score 사용)\n",
    "\n",
    "# 'ph' 컬럼의 평균과 표준편차 계산\n",
    "mean_ph = df.agg({'ph': 'mean'}).collect()[0][0]\n",
    "stddev_ph = df.agg({'ph': 'stddev'}).collect()[0][0]\n",
    "print(f\"ph 평균: {mean_ph}, ph 표준편차: {stddev_ph}\")\n",
    "\n",
    "# Z-score 계산 및 이상치 경계값 설정 (Z-score가 3보다 크면 이상치로 간주)\n",
    "upper_bound_ph = mean_ph + 3 * stddev_ph\n",
    "lower_bound_ph = mean_ph - 3 * stddev_ph\n",
    "print(f\"ph 상한선: {upper_bound_ph}, ph 하한선: {lower_bound_ph}\")\n",
    "\n",
    "# 이상치 제거\n",
    "df_filtered = df.filter((col('ph') >= lower_bound_ph) & (col('ph') <= upper_bound_ph))\n",
    "print(f\"원래 데이터 수: {df.count()}, 'ph' 이상치 제거 후 데이터 수: {df_filtered.count()}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# 2. Random Forest 모델 학습 및 평가\n",
    "\n",
    "# Feature와 Label 준비\n",
    "feature_cols = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df_filtered)\n",
    "\n",
    "# Label 인덱싱 (문자열 라벨을 숫자로 변환)\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\n",
    "df_indexed = indexer.fit(df_assembled).transform(df_assembled)\n",
    "\n",
    "# 데이터 분할 (훈련: 80%, 테스트: 20%)\n",
    "train_data, test_data = df_indexed.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Random Forest 모델 생성 및 학습\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"indexedLabel\", numTrees=100)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# 테스트 데이터에 모델 적용\n",
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "# 정확도 평가\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"모델 정확도 (Random Forest, ph Z-score 이상치 제거): {accuracy}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# 3. 작물별 정확도 분석\n",
    "\n",
    "# 작물별 평가를 위한 Evaluator 설정\n",
    "label_list = sorted(list(df.select(\"label\").distinct().rdd.flatMap(lambda x: x).collect()))\n",
    "indexer_model = indexer.fit(df_assembled)\n",
    "\n",
    "# 각 작물에 대한 정확도 계산\n",
    "print(\"\\n=== 작물별 정확도 ===\")\n",
    "for crop_label in label_list:\n",
    "    # 해당 작물의 인덱스 찾기\n",
    "    label_index = indexer_model.transform(spark.createDataFrame([(crop_label,)], [\"label\"])).select(\"indexedLabel\").collect()[0][0]\n",
    "\n",
    "    # 특정 작물에 대한 예측 필터링\n",
    "    crop_predictions = predictions.filter(col(\"indexedLabel\") == label_index)\n",
    "\n",
    "    # 해당 작물의 정확도 계산\n",
    "    if crop_predictions.count() > 0:\n",
    "        correct_predictions_count = crop_predictions.filter(col(\"indexedLabel\") == col(\"prediction\")).count()\n",
    "        accuracy_crop = correct_predictions_count / crop_predictions.count()\n",
    "        print(f\"작물 '{crop_label}': {accuracy_crop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "c680171d-de35-40cb-a157-239d8a0d86c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
